{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementry Syntax: RDD -> sparkDF -> pandasDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dummy dataset        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data as a pandasDF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_0</th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.250237</td>\n",
       "      <td>0.183555</td>\n",
       "      <td>0.215014</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.695688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.650195</td>\n",
       "      <td>0.166610</td>\n",
       "      <td>0.933383</td>\n",
       "      <td>0.089396</td>\n",
       "      <td>0.141637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.238586</td>\n",
       "      <td>0.977994</td>\n",
       "      <td>0.700689</td>\n",
       "      <td>0.227258</td>\n",
       "      <td>0.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148277</td>\n",
       "      <td>0.809591</td>\n",
       "      <td>0.955313</td>\n",
       "      <td>0.304549</td>\n",
       "      <td>0.329506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.797299</td>\n",
       "      <td>0.497570</td>\n",
       "      <td>0.636135</td>\n",
       "      <td>0.372165</td>\n",
       "      <td>0.207162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column_0  column_1  column_2  column_3  column_4\n",
       "0  0.250237  0.183555  0.215014  0.782768  0.695688\n",
       "1  0.650195  0.166610  0.933383  0.089396  0.141637\n",
       "2  0.238586  0.977994  0.700689  0.227258  0.829100\n",
       "3  0.148277  0.809591  0.955313  0.304549  0.329506\n",
       "4  0.797299  0.497570  0.636135  0.372165  0.207162"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROWS = 1000\n",
    "COLUMNS = 5\n",
    "\n",
    "\n",
    "data = []\n",
    "for i in range(ROWS*COLUMNS):\n",
    "    sample = random.random()\n",
    "    data.append(sample)\n",
    "\n",
    "    \n",
    "data = np.array(data)\n",
    "\n",
    "data = data.reshape((ROWS,COLUMNS))\n",
    "pandasDF = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "column_names = []\n",
    "for i in range(COLUMNS):\n",
    "    name = \"column_%s\" % i\n",
    "    column_names.append(name)\n",
    "\n",
    "pandasDF.columns = column_names\n",
    "\n",
    "print \"data as a pandasDF\"\n",
    "display(pandasDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up a Spark Session and create a sparkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "print first row of sparkDF:\n",
      "Row(column_0=0.2502368192953367, column_1=0.18355460368749033, column_2=0.21501365160371155, column_3=0.7827677590075888, column_4=0.6956882060822042)\n",
      "\n",
      "you can also do the .head() of a sparkDF:\n",
      "[Row(column_0=0.2502368192953367, column_1=0.18355460368749033, column_2=0.21501365160371155, column_3=0.7827677590075888, column_4=0.6956882060822042), Row(column_0=0.6501949358841297, column_1=0.1666102817563173, column_2=0.9333827070652461, column_3=0.08939629392444737, column_4=0.1416373280034805), Row(column_0=0.23858583884070117, column_1=0.9779938973558728, column_2=0.7006892024313864, column_3=0.22725765489006255, column_4=0.8291002106884638), Row(column_0=0.14827669390957376, column_1=0.809591003707794, column_2=0.9553127036225423, column_3=0.30454916266712373, column_4=0.32950633262575335), Row(column_0=0.7972988119377827, column_1=0.4975702867006666, column_2=0.6361347158481788, column_3=0.3721650352152236, column_4=0.2071617655452449)]\n",
      "\n",
      "you can also get a prettier view of a sparkDF with .show()\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|            column_0|            column_1|           column_2|           column_3|            column_4|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|  0.2502368192953367| 0.18355460368749033|0.21501365160371155| 0.7827677590075888|  0.6956882060822042|\n",
      "|  0.6501949358841297|  0.1666102817563173| 0.9333827070652461|0.08939629392444737|  0.1416373280034805|\n",
      "| 0.23858583884070117|  0.9779938973558728| 0.7006892024313864|0.22725765489006255|  0.8291002106884638|\n",
      "| 0.14827669390957376|   0.809591003707794| 0.9553127036225423|0.30454916266712373| 0.32950633262575335|\n",
      "|  0.7972988119377827|  0.4975702867006666| 0.6361347158481788| 0.3721650352152236|  0.2071617655452449|\n",
      "|  0.4518485795507049|  0.3303939741994649| 0.5945918618468496|0.35475246594596543|  0.6726835619260834|\n",
      "|  0.5702922917664767|  0.5523331521131793|0.28208888057181114| 0.2912649844188827|  0.4515645721973678|\n",
      "| 0.35163435406084087|  0.9159061859630107| 0.6360261783510858|0.27540747411261624|   0.801047737295205|\n",
      "|  0.5404611264884952|  0.5451660211713404| 0.6152090011470015| 0.5562950729434879|   0.771617722260397|\n",
      "|   0.659618623388003|  0.9152034369111041|   0.45946429153596|  0.927989118406828|  0.6728263931525281|\n",
      "|  0.9459466442312086|  0.6720131461865887| 0.8744685798978855| 0.6763800639041408|  0.9070549502239912|\n",
      "|    0.49963933077113| 0.03754953611192191| 0.3053925133554194| 0.4470019186782479| 0.21681876633578678|\n",
      "|   0.675823610809875| 0.19393920267186304| 0.4089629376913153| 0.9233186062506619|  0.8742015404597316|\n",
      "|  0.2887469562391187|  0.6719291738491073| 0.7923568824025184| 0.4661640421718307| 0.07242825857119028|\n",
      "|  0.7766781439009637|  0.9532762383092116|0.10631856269500595|0.09252350303475387|0.018534194838620066|\n",
      "|  0.8341881375046195|  0.5274012568437976|0.33185192052032675| 0.6299558776925932|0.051229152478533324|\n",
      "|0.010519112835449929| 0.21685332714547068| 0.6751286571051787|0.19793309306743756|  0.8190273115358944|\n",
      "|  0.9376927884639722|  0.9915297315202322|  0.848136986315447|  0.686732579339178| 0.05590243668572381|\n",
      "|   0.754668107916279|0.009193518251070731|0.24105355860048883|0.22800040877861738| 0.04065494382331869|\n",
      "| 0.09727728325864327|  0.2645312701580744| 0.2729506976273067| 0.4617688442457756| 0.49067438561544785|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "### Spark ###\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"app_spark\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Create a sparkDF from the pandasDF\n",
    "sparkDF = spark.createDataFrame(pandasDF)\n",
    "\n",
    "# .first() method returns the first row of a sparkDF\n",
    "print \"\\nprint first row of sparkDF:\"\n",
    "first = sparkDF.first()\n",
    "print first\n",
    "\n",
    "# .head(n) method returns the first n rows of a sparkDF\n",
    "print \"\\nyou can also do the .head() of a sparkDF:\"\n",
    "print sparkDF.head(5)\n",
    "\n",
    "print \"\\nyou can also get a prettier view of a sparkDF with .show()\"\n",
    "# .show() does not require a print statement for display\n",
    "sparkDF.show()\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert back and forth to RDDs and pandasDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original schema:\n",
      "StructType(List(StructField(column_0,DoubleType,true),StructField(column_1,DoubleType,true),StructField(column_2,DoubleType,true),StructField(column_3,DoubleType,true),StructField(column_4,DoubleType,true)))\n",
      "\n",
      "data_RDD before:\n",
      "[Row(column_0=0.2502368192953367, column_1=0.18355460368749033, column_2=0.21501365160371155, column_3=0.7827677590075888, column_4=0.6956882060822042), Row(column_0=0.6501949358841297, column_1=0.1666102817563173, column_2=0.9333827070652461, column_3=0.08939629392444737, column_4=0.1416373280034805), Row(column_0=0.23858583884070117, column_1=0.9779938973558728, column_2=0.7006892024313864, column_3=0.22725765489006255, column_4=0.8291002106884638), Row(column_0=0.14827669390957376, column_1=0.809591003707794, column_2=0.9553127036225423, column_3=0.30454916266712373, column_4=0.32950633262575335), Row(column_0=0.7972988119377827, column_1=0.4975702867006666, column_2=0.6361347158481788, column_3=0.3721650352152236, column_4=0.2071617655452449)]\n",
      "\n",
      "data_RDD after:\n",
      "[[0.0, 0.18355460368749033, 0.4300273032074231, 2.348303277022766, 2.7827528243288167], [0.0, 0.1666102817563173, 1.8667654141304921, 0.2681888817733421, 0.566549312013922], [0.0, 0.9779938973558728, 1.4013784048627729, 0.6817729646701877, 3.3164008427538554], [0.0, 0.809591003707794, 1.9106254072450846, 0.9136474880013712, 1.3180253305030134], [0.0, 0.4975702867006666, 1.2722694316963576, 1.116495105645671, 0.8286470621809796], [0.0, 0.3303939741994649, 1.1891837236936993, 1.0642573978378964, 2.6907342477043334], [0.0, 0.5523331521131793, 0.5641777611436223, 0.8737949532566481, 1.8062582887894711], [0.0, 0.9159061859630107, 1.2720523567021715, 0.8262224223378487, 3.20419094918082], [0.0, 0.5451660211713404, 1.230418002294003, 1.6688852188304637, 3.086470889041588], [0.0, 0.9152034369111041, 0.91892858307192, 2.7839673552204838, 2.6913055726101125]]\n",
      "+--------+-------------------+------------------+------------------+------------------+\n",
      "|column_0|           column_1|          column_2|          column_3|          column_4|\n",
      "+--------+-------------------+------------------+------------------+------------------+\n",
      "|     0.0|0.18355460368749033|0.4300273032074231| 2.348303277022766|2.7827528243288167|\n",
      "|     0.0| 0.1666102817563173|1.8667654141304921|0.2681888817733421| 0.566549312013922|\n",
      "|     0.0| 0.9779938973558728|1.4013784048627729|0.6817729646701877|3.3164008427538554|\n",
      "|     0.0|  0.809591003707794|1.9106254072450846|0.9136474880013712|1.3180253305030134|\n",
      "|     0.0| 0.4975702867006666|1.2722694316963576| 1.116495105645671|0.8286470621809796|\n",
      "+--------+-------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_0</th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183555</td>\n",
       "      <td>0.430027</td>\n",
       "      <td>2.348303</td>\n",
       "      <td>2.782753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166610</td>\n",
       "      <td>1.866765</td>\n",
       "      <td>0.268189</td>\n",
       "      <td>0.566549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.977994</td>\n",
       "      <td>1.401378</td>\n",
       "      <td>0.681773</td>\n",
       "      <td>3.316401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.809591</td>\n",
       "      <td>1.910625</td>\n",
       "      <td>0.913647</td>\n",
       "      <td>1.318025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497570</td>\n",
       "      <td>1.272269</td>\n",
       "      <td>1.116495</td>\n",
       "      <td>0.828647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column_0  column_1  column_2  column_3  column_4\n",
       "0       0.0  0.183555  0.430027  2.348303  2.782753\n",
       "1       0.0  0.166610  1.866765  0.268189  0.566549\n",
       "2       0.0  0.977994  1.401378  0.681773  3.316401\n",
       "3       0.0  0.809591  1.910625  0.913647  1.318025\n",
       "4       0.0  0.497570  1.272269  1.116495  0.828647"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make an RDD form a sparkDF \n",
    "data_RDD = sparkDF.rdd\n",
    "\n",
    "# pull the schema from the sparkDF for later\n",
    "schema = sparkDF.schema\n",
    "print \"The original schema:\\n\",schema\n",
    "\n",
    "print \"\\ndata_RDD before:\\n\", data_RDD.take(5)\n",
    "\n",
    "# Now we can work with the data in with mapper and reducer paradigm\n",
    "def mapper_function(row):\n",
    "    out_row = []\n",
    "    for i in range(len(row)):\n",
    "        out_row.append(row[i]*i)\n",
    "    return out_row\n",
    "\n",
    "new_RDD = data_RDD.map(lambda row: mapper_function(row))\n",
    "\n",
    "print \"\\ndata_RDD after:\\n\", new_RDD.take(10)\n",
    "\n",
    "# Convert the new RDD back to sparkDF \n",
    "# Make sure to load the 'schema=' argument\n",
    "new_sparkDF = new_RDD.toDF(schema=schema)\n",
    "\n",
    "# View the new sparkDF\n",
    "new_sparkDF.first()\n",
    "new_sparkDF.show(5)\n",
    "\n",
    "# convert the sparkDF to a pandasDF \n",
    "new_pandasDF = new_sparkDF.toPandas()\n",
    "\n",
    "display(new_pandasDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can use Pandas like syntax to filter sparkDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|          column_0|            column_1|           column_2|            column_3|            column_4|\n",
      "+------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|0.9993951595211199|0.029367447254841195| 0.6685370712082788|  0.6236610054053627| 0.13127968510727994|\n",
      "|0.9997610154240947|  0.6854425950558367| 0.6559377564159153|  0.9308906186030844|  0.6896996549583291|\n",
      "| 0.997127359988409|  0.9229308812309674| 0.1146010965843266| 0.18753752035377214| 0.18385179543655217|\n",
      "|0.9998557721426741|  0.4035282104235407|  0.486204908051453|  0.3896757218590855|  0.4724727281286535|\n",
      "|0.9922742665307508|  0.2811508406364178| 0.8241255306763826|  0.0588272205327377| 0.27403606754227827|\n",
      "|0.9914236271467993|  0.8820785921937543| 0.7244537217491043|  0.4583781816898216| 0.22388193649896304|\n",
      "|0.9929329614058017|  0.7312139862363859|0.21805864514244444|0.014614234529583214|  0.8983727776072739|\n",
      "|   0.9969514605863|  0.4933113983884959| 0.5799204749728032|  0.5009637143694933|  0.6758687675926432|\n",
      "|0.9945907798333171|  0.9409085694407031| 0.9742678020056833|  0.2145385020974382|  0.5701681216579898|\n",
      "|0.9999048079458933|   0.258911925549855|0.28284847056362583|0.030394935709256665|  0.7023938711818095|\n",
      "|0.9919302965047326|   0.746321990499317| 0.9530728486977772|  0.7322301132244959|  0.5552810778971294|\n",
      "|0.9906154493359749|  0.1109089855269918| 0.8763090756350026|  0.9444401474030375| 0.45556725195576864|\n",
      "|0.9900589669574061|  0.8273432251991315| 0.8429322011514442|  0.7182667500390232|0.013680767128162574|\n",
      "+------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF = sparkDF[sparkDF[\"column_0\"]>= .99]\n",
    "filteredDF.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: the `.select()` method is used for grabbing a single row\n",
    "* This is **different** form pandas syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           column_0|\n",
      "+-------------------+\n",
      "| 0.2502368192953367|\n",
      "| 0.6501949358841297|\n",
      "|0.23858583884070117|\n",
      "|0.14827669390957376|\n",
      "| 0.7972988119377827|\n",
      "| 0.4518485795507049|\n",
      "| 0.5702922917664767|\n",
      "|0.35163435406084087|\n",
      "| 0.5404611264884952|\n",
      "|  0.659618623388003|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF.select(\"column_0\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get summary statistics similar to pandas with the `describe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>column_0</th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.503540568335868</td>\n",
       "      <td>0.49105004488496623</td>\n",
       "      <td>0.5046307212741203</td>\n",
       "      <td>0.5018073505560066</td>\n",
       "      <td>0.49894873067603224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.2925918332707907</td>\n",
       "      <td>0.2896823828947518</td>\n",
       "      <td>0.2913106882049708</td>\n",
       "      <td>0.2847334662311207</td>\n",
       "      <td>0.291801714052591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1.3340337608869213E-4</td>\n",
       "      <td>3.7818412690193703E-4</td>\n",
       "      <td>8.000008504710499E-4</td>\n",
       "      <td>7.376032718799941E-4</td>\n",
       "      <td>4.699503681363515E-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>0.9999048079458933</td>\n",
       "      <td>0.9991836291434502</td>\n",
       "      <td>0.9990497752859389</td>\n",
       "      <td>0.9983956844507894</td>\n",
       "      <td>0.9998578469548808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary               column_0               column_1              column_2  \\\n",
       "0   count                   1000                   1000                  1000   \n",
       "1    mean      0.503540568335868    0.49105004488496623    0.5046307212741203   \n",
       "2  stddev     0.2925918332707907     0.2896823828947518    0.2913106882049708   \n",
       "3     min  1.3340337608869213E-4  3.7818412690193703E-4  8.000008504710499E-4   \n",
       "4     max     0.9999048079458933     0.9991836291434502    0.9990497752859389   \n",
       "\n",
       "               column_3              column_4  \n",
       "0                  1000                  1000  \n",
       "1    0.5018073505560066   0.49894873067603224  \n",
       "2    0.2847334662311207     0.291801714052591  \n",
       "3  7.376032718799941E-4  4.699503681363515E-4  \n",
       "4    0.9983956844507894    0.9998578469548808  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means [u'0.503540568335868', u'0.49105004488496623', u'0.5046307212741203', u'0.5018073505560066', u'0.49894873067603224']\n",
      "stddevs [u'0.2925918332707907', u'0.2896823828947518', u'0.2913106882049708', u'0.2847334662311207', u'0.291801714052591']\n"
     ]
    }
   ],
   "source": [
    "description = sparkDF.describe()\n",
    "display(description.toPandas())\n",
    "\n",
    "means = description[description[\"summary\"]==\"mean\"].collect()\n",
    "\n",
    "means = list(np.array(means)[0][1:])\n",
    "print \"means\", means\n",
    "\n",
    "stddevs = description[description[\"summary\"]==\"stddev\"].collect()\n",
    "\n",
    "stddevs = list(np.array(stddevs)[0][1:])\n",
    "print \"stddevs\", stddevs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can cash sparkDFs as with RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the BEFORE cashed status of the original sparkDF? False\n",
      "New cashedDF object is of type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "What is the BEFORE cashed status of cashedDF? False\n",
      "What is the AFTER cashed status of cashedDF? True\n",
      "What is the AFTER cashed status of the original sparkDF? False\n"
     ]
    }
   ],
   "source": [
    "# Check if sparkDF is Cashed\n",
    "print  \"What is the BEFORE cashed status of the original sparkDF?\", sparkDF.is_cached\n",
    "\n",
    "# Duplicate sparkDF as cashedDF using SQL style commands\n",
    "sparkDF.createOrReplaceTempView(\"cashedDF\")\n",
    "cashedDF = spark.sql(\"select * from cashedDF\")\n",
    "print \"New cashedDF object is of type:\", type(cashedDF)\n",
    "\n",
    "print \"What is the BEFORE cashed status of cashedDF?\", cashedDF.is_cached\n",
    "\n",
    "# use the .cashe method to cash the new DF\n",
    "cashedDF.cache()\n",
    "\n",
    "print \"What is the AFTER cashed status of cashedDF?\", cashedDF.is_cached\n",
    "\n",
    "\n",
    "print \"What is the AFTER cashed status of the original sparkDF?\", sparkDF.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Note: using the `newDF = sparkDF` will **NOT** make a new DF object called newDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the BEFORE cashed status of the original sparkDF? False\n",
      "What is the BEFORE cashed status of newDF? False\n",
      "What is the AFTER cashed status of newDF? True\n",
      "What is the AFTER cashed status of the original sparkDF? True\n"
     ]
    }
   ],
   "source": [
    "# Check if sparkDF is Cashed\n",
    "print  \"What is the BEFORE cashed status of the original sparkDF?\", sparkDF.is_cached\n",
    "\n",
    "newDF = sparkDF\n",
    "\n",
    "print \"What is the BEFORE cashed status of newDF?\", newDF.is_cached\n",
    "\n",
    "# use the .cashe method to cash the new DF\n",
    "newDF.cache()\n",
    "\n",
    "print \"What is the AFTER cashed status of newDF?\", newDF.is_cached\n",
    "\n",
    "# Cashing newDF cashes the original sparkDF since newDF is just a pointer asignment\n",
    "print \"What is the AFTER cashed status of the original sparkDF?\", sparkDF.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating sparkDFs using `.withColumn()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
      "|column_0_standardized|column_1_standardized|column_2_standardized|column_3_standardized|column_4_standardized|\n",
      "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
      "|  -0.8657239206192064|   -1.061491686599846|  -0.9941862121674047|   0.9867488081771296|   0.6742231657023032|\n",
      "|   0.5012250885773345|  -1.1199844460208246|   1.4718031406022634|  -1.4484109019238935|  -1.2245006984696918|\n",
      "|  -0.9055438305749445|   1.6809577703847016|   0.6730219284618099|  -0.9642340231379966|    1.131424059943891|\n",
      "|  -1.2141961395663667|   1.0996214393137191|   1.5470835798213822|  -0.6927818865129476|  -0.5806765001370179|\n",
      "|   1.0039864760334385|  0.02250824420366545|   0.4514217977526352|  -0.4553111267770895|  -0.9999494556697156|\n",
      "+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new sparkDF from orignal DF\n",
    "sparkDF.createOrReplaceTempView(\"sparkDF_standardized\")\n",
    "sparkDF_standardized = spark.sql(\"select * from sparkDF_standardized\")\n",
    "\n",
    "\n",
    "\n",
    "#sparkDF_standardized = sparkDF\n",
    "for i in range(COLUMNS):\n",
    "    new_name = \"column_%s_standardized\" % i\n",
    "    old_name = \"column_%s\" % i\n",
    "    sparkDF_standardized = sparkDF_standardized.withColumn(new_name, (sparkDF_standardized[old_name] - means[i])/float(stddevs[i]))\n",
    "    sparkDF_standardized = sparkDF_standardized.drop(old_name)\n",
    "\n",
    "sparkDF_standardized.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This can also be done with a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|column_0_standardized2|column_1_standardized2|column_2_standardized2|column_3_standardized2|column_4_standardized2|\n",
      "+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|   -0.8657239206192064|    -1.061491686599846|   -0.9941862121674047|    0.9867488081771296|    0.6742231657023032|\n",
      "|    0.5012250885773345|   -1.1199844460208246|    1.4718031406022634|   -1.4484109019238935|   -1.2245006984696918|\n",
      "|   -0.9055438305749445|    1.6809577703847016|    0.6730219284618099|   -0.9642340231379966|     1.131424059943891|\n",
      "|   -1.2141961395663667|    1.0996214393137191|    1.5470835798213822|   -0.6927818865129476|   -0.5806765001370179|\n",
      "|    1.0039864760334385|   0.02250824420366545|    0.4514217977526352|   -0.4553111267770895|   -0.9999494556697156|\n",
      "+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def standardize(x, mean, std):\n",
    "    return (x - mean)/float(std)\n",
    "\n",
    "\n",
    "sparkDF.createOrReplaceTempView(\"sparkDF_standardized2\")\n",
    "sparkDF_standardized2 = spark.sql(\"select * from sparkDF_standardized2\")\n",
    "\n",
    "for i in range(COLUMNS):\n",
    "    new_name = \"column_%s_standardized2\" % i\n",
    "    old_name = \"column_%s\" % i\n",
    "    sparkDF_standardized2 = sparkDF_standardized2.withColumn(new_name, standardize(sparkDF_standardized2[old_name], means[i], stddevs[i]))\n",
    "    sparkDF_standardized2 = sparkDF_standardized2.drop(old_name)\n",
    "\n",
    "sparkDF_standardized2.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
